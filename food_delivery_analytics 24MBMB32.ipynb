{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af3a940d-80fa-4140-ac53-c313627af9da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773fd914-6203-4e51-b738-bec15804e6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------+\n|database|tableName            |isTemporary|\n+--------+---------------------+-----------+\n|default |food_delivery_dataset|false      |\n|default |shyam                |false      |\n+--------+---------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN default\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f68eb58-6bec-420e-a839-281324e5cf87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c259ba25-f096-4178-b3aa-84b6a95b4af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+------------+-------------+---------------+-------------------+---------------------+-----------+-------------+------------+-------------------+\n|order_id|customer_id|customer_name|    locality|restaurant_id|restaurant_name|delivery_partner_id|delivery_partner_name|order_value|delivery_time|order_status|     date_timestamp|\n+--------+-----------+-------------+------------+-------------+---------------+-------------------+---------------------+-----------+-------------+------------+-------------------+\n|    O001|       C105|       Vikram|        KPHB|         R205|Tandoori Flames|               D304|                Manoj|        783|           23|   cancelled|2025-08-05 18:19:00|\n|    O002|       C109|         Ravi|BanjaraHills|         R201|  Biryani House|               D303|                Kavya|        738|           24|   delivered|2025-08-06 12:54:00|\n|    O003|       C104|        Sneha|  HitechCity|         R205|Tandoori Flames|               D304|                Manoj|        289|           33|   cancelled|2025-08-04 04:55:00|\n|    O004|       C107|      Karthik|     Miyapur|         R204|  Meals Express|               D302|                Arjun|        520|           45|   cancelled|2025-08-06 09:46:00|\n|    O005|       C106|        Divya|    Madhapur|         R203|   Burger Point|               D301|               Suresh|        627|           16|   delivered|2025-08-02 14:52:00|\n+--------+-----------+-------------+------------+-------------+---------------+-------------------+---------------------+-----------+-------------+------------+-------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset from the table you created\n",
    "orders_df = spark.table(\"default.food_delivery_dataset\")\n",
    "\n",
    "# Show first few rows\n",
    "orders_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099f164e-03f2-4d84-be79-5055a63eebda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n|customer_id|total_order_value|\n+-----------+-----------------+\n|C109       |3309             |\n|C102       |4362             |\n|C106       |4319             |\n|C107       |2418             |\n|C104       |2626             |\n|C105       |3886             |\n|C101       |2834             |\n|C103       |2367             |\n|C108       |2270             |\n|C110       |1814             |\n+-----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Total order value per customer\n",
    "customer_order_value = orders_df.groupBy(\"customer_id\") \\\n",
    "                                .agg(sum(\"order_value\").alias(\"total_order_value\"))\n",
    "\n",
    "# Show results\n",
    "customer_order_value.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b01893-08f1-4a51-ba57-99fff06881dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n|restaurant_id|avg_delivery_time|\n+-------------+-----------------+\n|R202         |37.55555555555556|\n|R203         |32.88235294117647|\n|R204         |34.84615384615385|\n|R205         |34.22222222222222|\n|R201         |33.0             |\n+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Average delivery time per restaurant\n",
    "avg_delivery_time = orders_df.groupBy(\"restaurant_id\") \\\n",
    "                             .agg(avg(\"delivery_time\").alias(\"avg_delivery_time\"))\n",
    "\n",
    "avg_delivery_time.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a588b84-d41a-4966-90be-6a3b05a93a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register DataFrame as a temp view\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b6855f-6402-4592-9521-88986205830a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n|customer_id|total_cancellations|\n+-----------+-------------------+\n|C106       |5                  |\n|C108       |4                  |\n|C105       |3                  |\n|C101       |3                  |\n|C104       |3                  |\n|C102       |3                  |\n|C107       |3                  |\n|C109       |2                  |\n|C103       |2                  |\n|C110       |2                  |\n+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Count cancellations per customer\n",
    "frequent_cancellations = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) AS total_cancellations\n",
    "    FROM orders\n",
    "    WHERE order_status = 'cancelled'\n",
    "    GROUP BY customer_id\n",
    "    HAVING COUNT(*) > 1\n",
    "    ORDER BY total_cancellations DESC\n",
    "\"\"\")\n",
    "\n",
    "frequent_cancellations.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e51af72-eb1c-46f7-8a1b-7f46a7ac34bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|order_hour|total_orders|\n+----------+------------+\n|22        |7           |\n|23        |5           |\n|21        |4           |\n|7         |4           |\n|9         |3           |\n|2         |3           |\n|17        |3           |\n|3         |3           |\n|10        |3           |\n|11        |3           |\n+----------+------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "peak_hours = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        HOUR(date_timestamp) AS order_hour,\n",
    "        COUNT(*) AS total_orders\n",
    "    FROM orders\n",
    "    GROUP BY HOUR(date_timestamp)\n",
    "    ORDER BY total_orders DESC\n",
    "\"\"\")\n",
    "\n",
    "peak_hours.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8fd4ea-0bfd-4448-b856-2cb2d039ee07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|order_hour|total_orders|\n+----------+------------+\n|        22|           7|\n|        23|           5|\n|         7|           4|\n|        21|           4|\n|         2|           3|\n|        11|           3|\n|         3|           3|\n|        18|           3|\n|         9|           3|\n|        14|           3|\n|        10|           3|\n|        17|           3|\n|         1|           2|\n|         8|           2|\n|        13|           2|\n|        19|           2|\n|        20|           2|\n|         6|           1|\n|        16|           1|\n|        15|           1|\n+----------+------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "peak_hours = spark.sql(\"\"\"\n",
    "    SELECT HOUR(date_timestamp) AS order_hour,\n",
    "           COUNT(*) AS total_orders\n",
    "    FROM orders\n",
    "    GROUP BY HOUR(date_timestamp)\n",
    "    ORDER BY total_orders DESC\n",
    "\"\"\")\n",
    "peak_hours.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "food_delivery_analytics 24MBMB32",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}